{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"machine_shape":"hm","gpuType":"A100"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"A1JeqemMSLA8"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["!pip install flask-ngrok\n","!pip install flask==0.12.2\n","!pip install pyngrok==4.1.1\n","!ngrok authtoken '2E0itmXyrnKa7DoJmLdkZxE4Hk3_2hreUgB64mTNMJs6RjKfZ'\n","!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.tgz\n","!tar -xvf /content/ngrok-stable-linux-amd64.tgz\n","!pip install mxnet\n","!pip install gluonnlp==0.8.0\n","!pip install pandas tqdm\n","!pip install sentencepiece\n","!pip install transformers\n","!pip install torch\n","!pip install 'git+https://github.com/SKTBrain/KoBERT.git#egg=kobert_tokenizer&subdirectory=kobert_hf'"],"metadata":{"id":"jIrcCJwpUp2w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","from torch import nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","import gluonnlp as nlp\n","import numpy as np\n","from tqdm import tqdm, tqdm_notebook\n","from kobert_tokenizer import KoBERTTokenizer\n","from transformers import BertModel\n","from transformers import AdamW\n","from transformers.optimization import get_cosine_schedule_with_warmup\n","\n","# GPU 사용\n","device = torch.device(\"cuda:0\")"],"metadata":{"id":"985GR7NsXyFI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if torch.cuda.is_available():\n","    device = torch.device(\"cuda\")\n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","    print('We will use the GPU:', torch.cuda.get_device_name(0))\n","else:\n","    device = torch.device(\"cpu\")\n","    print('No GPU available, using the CPU instead.')"],"metadata":{"id":"YTJmZgGmaIwg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class BERTClassifier(nn.Module):\n","    def __init__(self,\n","                 bert,\n","                 hidden_size = 768,\n","                 num_classes=7,\n","                 dr_rate=None,\n","                 params=None):\n","        super(BERTClassifier, self).__init__()\n","        self.bert = bert\n","        self.dr_rate = dr_rate\n","\n","        self.classifier = nn.Linear(hidden_size , num_classes)\n","        if dr_rate:\n","            self.dropout = nn.Dropout(p=dr_rate)\n","\n","    def gen_attention_mask(self, token_ids, valid_length):\n","        attention_mask = torch.zeros_like(token_ids)\n","        for i, v in enumerate(valid_length):\n","            attention_mask[i][:v] = 1\n","        return attention_mask.float()\n","\n","    def forward(self, token_ids, valid_length, segment_ids):\n","        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n","\n","        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n","        if self.dr_rate:\n","            out = self.dropout(pooler)\n","        return self.classifier(out)\n","\n","model = torch.load(f'/content/drive/MyDrive/Final_project/model.pt')\n","model.eval()\n","print(model)"],"metadata":{"id":"kz7a79rwXxEh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Setting parameters\n","max_len = 64\n","batch_size = 32\n","warmup_ratio = 0.1\n","num_epochs = 5\n","max_grad_norm = 1\n","log_interval = 200\n","learning_rate =  5e-5"],"metadata":{"id":"dPidhcWFlCQe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#토큰화\n","tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')\n","vocab = nlp.vocab.BERTVocab.from_sentencepiece(tokenizer.vocab_file, padding_token='[PAD]')\n","\n","def predict(predict_sentence):\n","\n","    data = [predict_sentence, '0']\n","    dataset_another = [data]\n","    emotion_dict = {0: 'fear', 1: 'surprise', 2: 'angry', 3: 'sad', 4: 'neutral', 5: 'happiness', 6: 'disgust'}\n","    another_test = BERTDataset(dataset_another, 0, 1, tokenizer, vocab, max_len, True, False)\n","\n","    test_dataloader = torch.utils.data.DataLoader(another_test, batch_size=batch_size, num_workers=5)\n","\n","    model.eval()\n","\n","    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(test_dataloader):\n","        token_ids = token_ids.long().to(device)\n","        segment_ids = segment_ids.long().to(device)\n","\n","        valid_length= valid_length\n","        label = label.long().to(device)\n","\n","        out = model(token_ids, valid_length, segment_ids)\n","\n","\n","#         test_eval=[]\n","        for i in out:\n","            logits=i\n","            logits = logits.detach().cpu().numpy()\n","            emotion = emotion_dict[np.argmax(logits)]\n","\n","\n","    return emotion\n","\n","\n","class BERTDataset(Dataset):\n","    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, vocab, max_len, pad, pair):\n","        transform = BERTSentenceTransform(bert_tokenizer, max_seq_length=max_len, vocab=vocab, pad=pad, pair=pair)\n","        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n","        self.labels = [np.int32(i[label_idx]) for i in dataset]\n","\n","    def __getitem__(self, i):\n","        return (self.sentences[i] + (self.labels[i], ))\n","\n","    def __len__(self):\n","        return (len(self.labels))\n","\n","class BERTSentenceTransform:\n","\n","    def __init__(self, tokenizer, max_seq_length, vocab, pad=True, pair=True):\n","        self._tokenizer = tokenizer\n","        self._max_seq_length = max_seq_length\n","        self._pad = pad\n","        self._pair = pair\n","        self._vocab = vocab ##추가\n","\n","    def __call__(self, line):\n","\n","\n","        # convert to unicode\n","        text_a = line[0]\n","        if self._pair:\n","            assert len(line) == 2\n","            text_b = line[1]\n","\n","        #tokens_a = self._tokenizer(text_a)\n","        tokens_a = self._tokenizer.tokenize(text_a)\n","        tokens_b = None\n","\n","        if self._pair:\n","            tokens_b = self._tokenizer(text_b)\n","\n","        if tokens_b:\n","            # Modifies `tokens_a` and `tokens_b` in place so that the total\n","            # length is less than the specified length.\n","            # Account for [CLS], [SEP], [SEP] with \"- 3\"\n","            self._truncate_seq_pair(tokens_a, tokens_b,\n","                                    self._max_seq_length - 3)\n","        else:\n","            # Account for [CLS] and [SEP] with \"- 2\"\n","            if len(tokens_a) > self._max_seq_length - 2:\n","                tokens_a = tokens_a[0:(self._max_seq_length - 2)]\n","\n","\n","        #vocab = self._tokenizer.vocab\n","        vocab = self._vocab\n","        tokens = []\n","        tokens.append(vocab.cls_token)\n","        tokens.extend(tokens_a)\n","        tokens.append(vocab.sep_token)\n","        segment_ids = [0] * len(tokens)\n","\n","        if tokens_b:\n","            tokens.extend(tokens_b)\n","            tokens.append(vocab.sep_token)\n","            segment_ids.extend([1] * (len(tokens) - len(segment_ids)))\n","\n","        input_ids = self._tokenizer.convert_tokens_to_ids(tokens)\n","\n","        #input_ids = tokens_a['input_ids']\n","\n","        # The valid length of sentences. Only real  tokens are attended to.\n","        valid_length = len(input_ids)\n","\n","        if self._pad:\n","            # Zero-pad up to the sequence length.\n","            padding_length = self._max_seq_length - valid_length\n","            # use padding tokens for the rest\n","            input_ids.extend([vocab[vocab.padding_token]] * padding_length)\n","            segment_ids.extend([0] * padding_length)\n","\n","        return np.array(input_ids, dtype='int32'), np.array(valid_length, dtype='int32'),\\\n","            np.array(segment_ids, dtype='int32')\n","\n"],"metadata":{"id":"I3mMIQv3Y1p4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["user_input = '안녕하세요. 오늘은 너무 슬프다. 요즘 식욕이 없고 살이 빠진다.'\n","sentences = [sentence for sentence in user_input.split('.') if sentence]\n","emotions = []\n","\n","for sentence in sentences:\n","    result = predict(sentence)\n","    emotions.append(result)\n","\n","print(emotions)\n"],"metadata":{"id":"GaL42kKJkr53"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Flask API 서버 실행"],"metadata":{"id":"Q8CXCP0rYI2I"}},{"cell_type":"code","source":["!pip install -U flask jinja2"],"metadata":{"id":"q4dGv6P0cLan"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from flask import Flask, jsonify, request\n","from flask_ngrok import run_with_ngrok\n","import requests\n","\n","app = Flask(__name__)\n","app.config['JSONIFY_PRETTYPRINT_REGULAR'] = False\n","run_with_ngrok(app)  # Start ngrok when app is run\n","\n","@app.route('/', methods=['POST'])\n","def get_emotions():\n","    data = request.get_json()  # POST 요청으로 전달된 JSON 데이터를 가져옵니다.\n","\n","    user_input = data['user_input']  # 'text'는 POST 요청에서 전달된 텍스트 필드의 키입니다.\n","\n","    sentences = [sentence for sentence in user_input.split('.') if sentence]\n","    emotions = []\n","\n","    for sentence in sentences:\n","      result = predict(sentence)\n","      emotions.append(result)\n","\n","    return jsonify({'emotions': emotions})  # 감정(emotions)을 JSON 형태로 반환합니다.\n","\n","if __name__ == '__main__':\n","    app.run()\n","\n","import threading\n","threading.Thread(target=app.run, kwargs={'host':'0.0.0.0','port':80}).start()"],"metadata":{"id":"NScO7Sj_SP-B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"yRG9Wzc4SQAJ"},"execution_count":null,"outputs":[]}]}
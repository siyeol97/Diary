{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff2807fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 라이브러리 import \n",
    "import matplotlib.pyplot as plt\n",
    "import librosa.display\n",
    "import librosa\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import multiprocessing\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import parselmouth\n",
    "\n",
    "\n",
    "from os.path import join\n",
    "from tensorflow import keras\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Conv1D, Conv2D, MaxPooling1D, MaxPooling2D, Bidirectional, Flatten, GRU, Dense, Dropout, GlobalAveragePooling1D, BatchNormalization\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, BatchNormalization, Dropout, ReLU, Softmax\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34558cbc",
   "metadata": {},
   "source": [
    "# 우울증 탐지 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "339bbd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 전처리\n",
    "\n",
    "def split_audio(filename, chunk_length=5):\n",
    "    # Load the audio file\n",
    "    y, sr = librosa.load(filename, sr=44100)\n",
    "\n",
    "    # Calculate the number of 1-minute chunks\n",
    "    total_length = librosa.get_duration(y=y, sr=sr)\n",
    "    num_chunks = int(total_length / chunk_length)\n",
    "\n",
    "    # Split the audio\n",
    "    audio_chunks = []\n",
    "    for i in range(num_chunks):\n",
    "        start = i * chunk_length * sr\n",
    "        end = (i+1) * chunk_length * sr\n",
    "        audio_chunk = y[start:end]\n",
    "        audio_chunks.append(audio_chunk)\n",
    "\n",
    "    # If there are any leftovers, pad and add them as well\n",
    "    if total_length > chunk_length * num_chunks:\n",
    "        start = num_chunks * chunk_length * sr\n",
    "        audio_chunk = np.pad(y[start:], (0, start + chunk_length * sr - len(y)))\n",
    "        audio_chunks.append(audio_chunk)\n",
    "\n",
    "    return audio_chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60f70bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특징 추출\n",
    "def extract_features(audio_file):\n",
    "    sr = 44100\n",
    "    # Preemphasis\n",
    "    y_pre = librosa.effects.preemphasis(audio_file, coef=0.97)\n",
    "\n",
    "    # MFCC and Mel Spectrogram parameters\n",
    "    n_fft = 1024\n",
    "    hop_length = 256\n",
    "    win_length = 512\n",
    "    window = 'hamming'\n",
    "    n_mels = 128\n",
    "    n_mfcc = 64\n",
    "\n",
    "    # Extract MFCC features\n",
    "    mfccs = librosa.feature.mfcc(y=y_pre, win_length = win_length , sr=sr, n_mfcc=n_mfcc, n_mels=n_mels, n_fft=n_fft, hop_length=hop_length, window=window)\n",
    "\n",
    "    # Extract Mel Spectrogram features\n",
    "    mel_spectrogram = librosa.feature.melspectrogram(y=y_pre, sr=sr, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels, window=window, win_length=win_length)\n",
    "    mel_spectrogram_db = librosa.power_to_db(mel_spectrogram)\n",
    "\n",
    "    # Load audio file with Parselmouth\n",
    "    sound = parselmouth.Sound(audio_file)\n",
    "\n",
    "    # Extract pitch\n",
    "    pitch = sound.to_pitch(time_step=hop_length/sr)\n",
    "    pitch_values = pitch.selected_array['frequency']\n",
    "\n",
    "    # Extract intensity\n",
    "    intensity = sound.to_intensity(time_step=hop_length/sr)\n",
    "    intensity_values = intensity.values[0]\n",
    "\n",
    "    # Interpolate pitch and intensity to match the length of MFCCs\n",
    "    pitch_interp = np.interp(np.linspace(0, len(pitch_values), mfccs.shape[1]), np.arange(len(pitch_values)), pitch_values)\n",
    "    intensity_interp = np.interp(np.linspace(0, len(intensity_values), mfccs.shape[1]), np.arange(len(intensity_values)), intensity_values)\n",
    "\n",
    "    # Normalize MFCCs, pitch, intensity and Mel Spectrogram\n",
    "    mfccs = (mfccs - np.min(mfccs)) / (np.max(mfccs) - np.min(mfccs))\n",
    "    mel_spectrogram_db = (mel_spectrogram_db - np.min(mel_spectrogram_db)) / (np.max(mel_spectrogram_db) - np.min(mel_spectrogram_db))\n",
    "\n",
    "    if not np.isnan(pitch_interp).all() and np.nanmin(pitch_interp) != np.nanmax(pitch_interp):\n",
    "        pitch_interp = (pitch_interp - np.nanmin(pitch_interp)) / (np.nanmax(pitch_interp) - np.nanmin(pitch_interp))\n",
    "    else:\n",
    "        pitch_interp = np.zeros_like(pitch_interp)\n",
    "\n",
    "    intensity_interp = (intensity_interp - np.min(intensity_interp)) / (np.max(intensity_interp) - np.min(intensity_interp))\n",
    "\n",
    "    # Stack MFCCs, Mel Spectrogram, pitch and intensity features\n",
    "    features = np.vstack([mfccs, mel_spectrogram_db, pitch_interp, intensity_interp])\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f083513c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# 1. conv block\n",
    "model.add(Conv2D(16, (3,3), padding='same', activation='relu', input_shape=(194, 862, 1)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# 2. conv block\n",
    "model.add(Conv2D(32, (3,3), padding='same', activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# 3. conv block\n",
    "model.add(Conv2D(64, (3,3), padding='same', activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# 4. conv block\n",
    "model.add(Conv2D(128, (3,3), padding='same', activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# flatten the output of the conv block\n",
    "model.add(Flatten())\n",
    "\n",
    "# output layer\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc9649ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_53.h5\t0.703781903\t0.910915911\t1.434558749\t0.830043495\t0.080872416\n",
    "\n",
    "model.load_weights(\"C:\\pknu_6\\model_w2_weight/model_53.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a0a259d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio_depress(user_audio):\n",
    "    features = [] \n",
    "    audio_chunks = split_audio(user_audio)\n",
    "    for i in range(len(audio_chunks)):\n",
    "        features.append(extract_features(audio_chunks[i]))\n",
    "    \n",
    "    prediction_sum = 0\n",
    "    for i in range(len(features)):\n",
    "        # Reshape the feature to match the input shape that model expects\n",
    "        # 모델이 받아들일 수 있는 형태로 차원을 변경\n",
    "        feature = np.expand_dims(features[i], axis=0)  # Add a dimension for batch size\n",
    "        prediction = model.predict(feature)\n",
    "        prediction_sum += prediction\n",
    "        \n",
    "        \n",
    "                     \n",
    "    predicted_class  = ((prediction_sum/len(features)) > 0.5).astype(int)\n",
    "    sigmoid_value = (prediction_sum/len(features))\n",
    "\n",
    "    dep_dict = { 0:'비우울', 1:'우울'}\n",
    "    \n",
    "    \n",
    "    return (dep_dict[int(predicted_class)], sigmoid_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07e545f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_audio = 'D:\\pknu_6\\daic-woz!\\interim\\P331\\P331_no_silence.wav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b610efe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# audio_depress(user_audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc23eca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a4cb1dfe",
   "metadata": {},
   "source": [
    "# 감정 탐지 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "746be279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 전처리\n",
    "\n",
    "def split_audio_35(filename, chunk_length=3):\n",
    "    # Load the audio file\n",
    "    y, sr = librosa.load(filename, sr=22050)\n",
    "\n",
    "    # Calculate the number of 1-minute chunks\n",
    "    total_length = librosa.get_duration(y=y, sr=sr)\n",
    "    num_chunks = int(total_length / chunk_length)\n",
    "\n",
    "    # Split the audio\n",
    "    audio_chunks = []\n",
    "    for i in range(num_chunks):\n",
    "        start = i * chunk_length * sr\n",
    "        end = (i+1) * chunk_length * sr\n",
    "        audio_chunk = y[start:end]\n",
    "        audio_chunks.append(audio_chunk)\n",
    "\n",
    "    # If there are any leftovers, pad and add them as well\n",
    "    if total_length > chunk_length * num_chunks:\n",
    "        start = num_chunks * chunk_length * sr\n",
    "        audio_chunk = np.pad(y[start:], (0, start + chunk_length * sr - len(y)))\n",
    "        audio_chunks.append(audio_chunk)\n",
    "\n",
    "    return audio_chunks\n",
    "\n",
    "### 정규화\n",
    "def nor(audio_np):\n",
    "    normed_wav = audio_np / max(np.abs(audio_np))\n",
    "    return normed_wav\n",
    "\n",
    "def extract_mfccs(audio_file):\n",
    "    \n",
    "    y_pre = librosa.effects.preemphasis(audio_file, coef=0.97)\n",
    "    \n",
    "    n_fft = 1024\n",
    "    hop_length = 256\n",
    "    win_length = 512\n",
    "    window = 'hamming'\n",
    "    n_mels = 128\n",
    "    n_mfcc = 64\n",
    "\n",
    "    mfccs = librosa.feature.mfcc(y=y_pre,win_length = win_length , sr=22050, n_mfcc=n_mfcc, n_mels=n_mels, n_fft=n_fft, hop_length=hop_length, window=window)\n",
    "    return mfccs\n",
    "\n",
    "                     \n",
    "                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a265f41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d79c1aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_audio_file(file_path):\n",
    "    audio_nor = nor(file_path)\n",
    "    mfcc = extract_mfccs(audio_nor)\n",
    "    audio_seq = pad_sequences(mfcc, padding='post' , truncating='post', maxlen=300, dtype='float32')\n",
    "    return audio_seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b0ff79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2eec2f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, BatchNormalization, Dropout, ReLU, Softmax\n",
    "\n",
    "model1 = Sequential()\n",
    "\n",
    "# 1. conv block\n",
    "model1.add(Conv2D(16, (3,3), padding='same', activation='relu', input_shape=(64, 300, 1)))\n",
    "model1.add(BatchNormalization())\n",
    "model1.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model1.add(Dropout(0.3))\n",
    "\n",
    "# 2. conv block\n",
    "model1.add(Conv2D(32, (3,3), padding='same', activation='relu'))\n",
    "model1.add(BatchNormalization())\n",
    "model1.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model1.add(Dropout(0.3))\n",
    "\n",
    "# 3. conv block\n",
    "model1.add(Conv2D(64, (3,3), padding='same', activation='relu'))\n",
    "model1.add(BatchNormalization())\n",
    "model1.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model1.add(Dropout(0.3))\n",
    "\n",
    "# 4. conv block\n",
    "model1.add(Conv2D(128, (3,3), padding='same', activation='relu'))\n",
    "model1.add(BatchNormalization())\n",
    "model1.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model1.add(Dropout(0.3))\n",
    "\n",
    "# flatten the output of the conv block\n",
    "model1.add(Flatten())\n",
    "\n",
    "# output layer\n",
    "model1.add(Dense(7, activation='softmax'))\n",
    "\n",
    "model1.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7415fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.load_weights(\"C:/pknu_6/model5_weight/model_15.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "884964ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio_emo(user_audio):\n",
    "    emotions = ['기쁨', '슬픔', '분노', '불안', '상처', '당황', '중립'] # 이 부분은 당신의 모델에 따라 다를 수 있음\n",
    "    \n",
    "    features = [] \n",
    "    audio_chunks = split_audio_35(user_audio)\n",
    "    for i in range(len(audio_chunks)):\n",
    "        features.append(preprocess_audio_file(audio_chunks[i]))\n",
    "    \n",
    "    predictions = []\n",
    "    for i in range(len(features)):\n",
    "        # Reshape the feature to match the input shape that model expects\n",
    "        feature = np.expand_dims(features[i], axis=0)  # Add a dimension for batch size\n",
    "        prediction = model1.predict(feature)\n",
    "        predictions.append(prediction)\n",
    "    \n",
    "    # Convert list of predictions to numpy array\n",
    "    predictions = np.array(predictions)\n",
    "\n",
    "    # Calculate average prediction\n",
    "    average_prediction = np.mean(predictions, axis=0)\n",
    "\n",
    "    # Find the emotion with the highest average prediction\n",
    "    max_index = np.argmax(average_prediction)\n",
    "    dominant_emotion = emotions[max_index]\n",
    "\n",
    "    return average_prediction, dominant_emotion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c407584d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# {'기쁨': 0, '슬픔': 1, '분노': 2, '불안': 3, '상처': 4, '당황': 5, '중립': 6}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "77e93cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from flask import Flask, jsonify, request\n",
    "# from flask_ngrok import run_with_ngrok\n",
    "# import requests\n",
    "# from pyngrok import ngrok\n",
    "\n",
    "# app = Flask(__name__)\n",
    "# app.config['JSONIFY_PRETTYPRINT_REGULAR'] = False\n",
    "# ngrok.kill()\n",
    "# run_with_ngrok(app)  # Start ngrok when app is run\n",
    "\n",
    "# @app.route('/', methods=['POST'])\n",
    "# def get_depress():\n",
    "#     data = request.get_json()  # POST 요청으로 전달된 JSON 데이터를 가져옵니다.\n",
    "\n",
    "#     user_input = data['user_input']  # 'text'는 POST 요청에서 전달된 텍스트 필드의 키입니다.\n",
    "\n",
    "#     depress_list = audio_depress(user_input)   # ['일상', '우울', '피로', '무기력', '식욕저하', '우울'] 와 같은 리스트 형식\n",
    "    \n",
    "#     return jsonify({'depress': depress_list})  # 감정(emotions)을 JSON 형태로 반환합니다.\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     app.run()  \n",
    "\n",
    "# import threading\n",
    "# threading.Thread(target=app.run, kwargs={'host':'0.0.0.0','port':80}).start() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0f2736",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "15ad0908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from flask import Flask, jsonify, request\n",
    "# from pyngrok import ngrok\n",
    "# import requests\n",
    "\n",
    "# app = Flask(__name__)\n",
    "# app.config['JSONIFY_PRETTYPRINT_REGULAR'] = False\n",
    "# ngrok.kill()\n",
    "\n",
    "\n",
    "# @app.route('/', methods=['POST'])\n",
    "# def get_depress():\n",
    "#     data = request.get_json()  # POST 요청으로 전달된 JSON 데이터를 가져옵니다.\n",
    "\n",
    "#     user_input = data['user_input']  # 'text'는 POST 요청에서 전달된 텍스트 필드의 키입니다.\n",
    "\n",
    "#     depress_list = audio_depress(file_path)   # ['일상', '우울', '피로', '무기력', '식욕저하', '우울'] 와 같은 리스트 형식\n",
    "    \n",
    "#     return jsonify({'depress': depress_list})  # 감정(emotions)을 JSON 형태로 반환합니다.\n",
    "\n",
    "# def start_ngrok():\n",
    "#     ngrok_tunnel_url = ngrok.connect(80)\n",
    "#     print('Public URL:', ngrok_tunnel_url)\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     start_ngrok()\n",
    "#     app.run(port=80)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299dc1ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "81b09b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from flask import Flask, jsonify, request\n",
    "# from pyngrok import ngrok\n",
    "# from datetime import datetime\n",
    "# import os\n",
    "\n",
    "# app = Flask(__name__)\n",
    "# app.config['JSONIFY_PRETTYPRINT_REGULAR'] = False\n",
    "# ngrok.kill()\n",
    "\n",
    "\n",
    "# def generate_unique_filename():\n",
    "#     current_date = datetime.now().strftime('%Y%m%d')\n",
    "#     timestamp = datetime.now().strftime('%H%M%S')\n",
    "#     return f'file_{current_date}_{timestamp}.wav'\n",
    "\n",
    "# @app.route('/', methods=['POST'])\n",
    "# def upload():\n",
    "#     if 'file' not in request.files:\n",
    "#         return jsonify({'error': 'No audio file found'}), 400\n",
    "\n",
    "#     file = request.files['file']\n",
    "\n",
    "#     # 저장할 파일 경로\n",
    "#     file_name = generate_unique_filename()\n",
    "#     file_path = os.path.join('./audiofiles', file_name)\n",
    "\n",
    "#     # 파일 저장\n",
    "#     file.save(file_path)\n",
    "\n",
    "#     # 오디오 파일 처리 후 결과 반환\n",
    "#     depress_list, sigmoid_value = audio_depress(file_path)\n",
    "    \n",
    "#     average_prediction, dominant_emotion = audio_emo(file_path)\n",
    "    \n",
    "    \n",
    "#     return jsonify({'depress': depress_list,\n",
    "#                    'sigmoid_value':sigmoid_value.tolist(),\n",
    "#                     'emotion': dominant_emotion,\n",
    "#                     'softmax_value':average_prediction.tolist()\n",
    "#                    })\n",
    "\n",
    "# def start_ngrok():\n",
    "#     ngrok_tunnel_url = ngrok.connect(80)\n",
    "#     print('Public URL:', ngrok_tunnel_url)\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     start_ngrok()\n",
    "#     app.run(port=80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312d3e2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Public URL: http://257e-175-214-183-100.ngrok-free.app\n",
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:80\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [20/Jun/2023 09:13:13] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [20/Jun/2023 09:13:13] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [20/Jun/2023 09:13:15] \"GET /favicon.ico HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [20/Jun/2023 09:13:16] \"GET /favicon.ico HTTP/1.1\" 404 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [20/Jun/2023 09:18:04] \"POST / HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [20/Jun/2023 09:22:16] \"POST / HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [20/Jun/2023 09:23:14] \"POST / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [20/Jun/2023 10:18:02] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [20/Jun/2023 10:18:03] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [20/Jun/2023 11:26:38] \"GET / HTTP/1.1\" 200 -\n",
      "t=2023-06-21T08:38:21+0900 lvl=eror msg=\"heartbeat timeout, terminating session\" obj=csess id=95aa9b3cc471 clientid=67e86907d9155d4c7281908a22b81f66\n",
      "\n",
      "t=2023-06-21T08:38:21+0900 lvl=eror msg=\"session closed, starting reconnect loop\" obj=csess id=cbdf611dfbab err=\"session closed\"\n",
      "\n",
      "t=2023-06-21T08:38:21+0900 lvl=eror msg=\"failed to reconnect session\" obj=csess id=cbdf611dfbab err=\"dial tcp: lookup tunnel.us.ngrok.com: no such host\"\n",
      "\n",
      "t=2023-06-21T08:38:21+0900 lvl=warn msg=\"failed to check for update\" obj=updater err=\"Post \\\"https://update.equinox.io/check\\\": dial tcp: lookup update.equinox.io: no such host\"\n",
      "\n",
      "t=2023-06-21T08:38:22+0900 lvl=eror msg=\"failed to reconnect session\" obj=csess id=cbdf611dfbab err=\"dial tcp: lookup tunnel.us.ngrok.com: no such host\"\n",
      "\n",
      "Exception in thread Thread-170:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\inkwabusan\\anaconda3\\envs\\pknu_nltk\\lib\\threading.py\", line 980, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\inkwabusan\\anaconda3\\envs\\pknu_nltk\\lib\\threading.py\", line 917, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\inkwabusan\\anaconda3\\envs\\pknu_nltk\\lib\\site-packages\\pyngrok\\process.py\", line 152, in _monitor_process\n",
      "    self._log_line(self.proc.stdout.readline())\n",
      "  File \"C:\\Users\\inkwabusan\\anaconda3\\envs\\pknu_nltk\\lib\\site-packages\\pyngrok\\process.py\", line 112, in _log_line\n",
      "    log = NgrokLog(line)\n",
      "  File \"C:\\Users\\inkwabusan\\anaconda3\\envs\\pknu_nltk\\lib\\site-packages\\pyngrok\\process.py\", line 209, in __init__\n",
      "    key, value = i.split(\"=\")\n",
      "ValueError: too many values to unpack (expected 2)\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, jsonify, request\n",
    "from pyngrok import ngrok\n",
    "from datetime import datetime\n",
    "from flask import make_response\n",
    "import os\n",
    "\n",
    "app = Flask(__name__)\n",
    "app.config['JSONIFY_PRETTYPRINT_REGULAR'] = False\n",
    "\n",
    "ngrok.kill()\n",
    "\n",
    "\n",
    "def generate_unique_filename():\n",
    "    current_date = datetime.now().strftime('%Y%m%d')\n",
    "    timestamp = datetime.now().strftime('%H%M%S')\n",
    "    return f'file_{current_date}_{timestamp}.wav'\n",
    "\n",
    "@app.route('/', methods=['GET','POST'])\n",
    "def upload():\n",
    "    \n",
    "    if request.method == 'POST':\n",
    "    \n",
    "        if 'file' not in request.files:\n",
    "            return jsonify({'error': 'No audio file found'}), 400\n",
    "\n",
    "        file = request.files['file']\n",
    "\n",
    "        # 저장할 파일 경로\n",
    "        file_name = generate_unique_filename()\n",
    "        file_path = os.path.join('./audiofiles', file_name)\n",
    "\n",
    "        # 파일 저장\n",
    "        file.save(file_path)\n",
    "\n",
    "        # 오디오 파일 처리 후 결과 반환\n",
    "        depress_list, sigmoid_value = audio_depress(file_path)\n",
    "\n",
    "        average_prediction, dominant_emotion = audio_emo(file_path)\n",
    "\n",
    "\n",
    "        return jsonify({'depress': depress_list,\n",
    "                       'sigmoid_value':sigmoid_value.tolist(),\n",
    "                        'emotion': dominant_emotion,\n",
    "                        'softmax_value':average_prediction.tolist()\n",
    "                       })\n",
    "    \n",
    "    else:\n",
    "        response = make_response('This is a GET request.')\n",
    "        response.headers['Cache-Control'] = 'no-cache, no-store, must-revalidate'\n",
    "        return response\n",
    "\n",
    "def start_ngrok():\n",
    "    ngrok_tunnel_url = ngrok.connect(80)\n",
    "    print('Public URL:', ngrok_tunnel_url)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    start_ngrok()\n",
    "    app.run(port=80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66572307",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e863ac4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pknu_nltk_kernel",
   "language": "python",
   "name": "pknu_nltk"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
